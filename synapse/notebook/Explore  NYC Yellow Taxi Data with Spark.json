{
	"name": "Explore  NYC Yellow Taxi Data with Spark",
	"properties": {
		"folder": {
			"name": "Module 01"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SampleSpark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "799aabb4-7f25-438b-9bb1-d747ce3dfaf9"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/073a4913-dc63-46b9-b676-fad837dab1ba/resourceGroups/rg-dp-500-platform/providers/Microsoft.Synapse/workspaces/synapse-6oq3z6xmzhbbc/bigDataPools/SampleSpark",
				"name": "SampleSpark",
				"type": "Spark",
				"endpoint": "https://synapse-6oq3z6xmzhbbc.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SampleSpark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Explore NYC Yellow Taxi Data using Spark\n",
					"\n",
					"In this notebook, you'll learn the basic steps to load and analyze an Open Dataset that tracks NYC Yellow Taxi trips with Apache Spark for Azure Synapse.\n",
					"\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Load Data\n",
					"\n",
					"Read NYC Yellow Taxi data as a Spark DataFrame object to manipulate."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Read NYC yellow cab data from Azure Open Datasets\n",
					"from azureml.opendatasets import NycTlcYellow\n",
					"\n",
					"from datetime import datetime\n",
					"from dateutil import parser\n",
					"\n",
					"end_date = parser.parse('2018-05-08 00:00:00')\n",
					"start_date = parser.parse('2018-05-01 00:00:00')\n",
					"\n",
					"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
					"df_nyc_tlc = nyc_tlc.to_spark_dataframe()"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Analyze the NYC Taxi data using Spark and notebooks\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"df_nyc_tlc.printSchema()"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"AvgTripDistance"
							],
							"values": [
								"passengerCount"
							],
							"yLabel": "passengerCount",
							"xLabel": "AvgTripDistance",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{\"passengerCount\":{\"2.382\":7,\"2.9365876998482907\":0,\"2.955385293728598\":1,\"3.0823106614325835\":6,\"3.1096431007047065\":5,\"3.124120509875713\":3,\"3.132080374155551\":4,\"3.1983281312300624\":2,\"6.23\":9,\"7.831666666666666\":8}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F\n",
					"df_nyc = df_nyc_tlc.groupBy(\"passengerCount\").agg(F.avg('tripDistance').alias('AvgTripDistance'), F.sum('tripDistance').alias('SumTripDistance'))\n",
					"display(df_nyc)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Customize data visualization with Spark and notebooks\n",
					"You can control how charts render by using notebooks. The following code shows a simple example. It uses the popular libraries matplotlib and seaborn. The code renders the same kind of line chart as the SQL queries we ran earlier.\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import matplotlib.pyplot\n",
					"import seaborn\n",
					"\n",
					"seaborn.set(style = \"whitegrid\")\n",
					"pdf_nyc = df_nyc.toPandas()\n",
					"seaborn.lineplot(x=\"passengerCount\", y=\"SumTripDistance\" , data = pdf_nyc)\n",
					"seaborn.lineplot(x=\"passengerCount\", y=\"AvgTripDistance\" , data = pdf_nyc)\n",
					"matplotlib.pyplot.show()"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Share data between spark and serverless sql"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"CREATE DATABASE IF NOT EXISTS NYCAggregates\")\r\n",
					"df_nyc.write.mode(\"overwrite\").saveAsTable(\"NYCAggregates.PassangerCountAggregates\")"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Clean up resources\n",
					"To ensure the Spark instance is shut down, end any connected sessions(notebooks). The pool shuts down when the **idle time** specified in the Apache Spark pool is reached. You can also select **stop session** from the status bar at the upper right of the notebook.\n",
					"\n",
					"![stopsession](https://adsnotebookrelease.blob.core.windows.net/adsnotebookrelease/adsnotebook/image/stopsession.png)"
				]
			}
		]
	}
}